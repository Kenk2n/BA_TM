{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text Mining \n",
    "\n",
    "> 비정형 데이터를 자연어 처리 방법을 통해서 데이터로 바꾼다\n",
    "> - 자연어처리\n",
    "> - 통계학 선형대수\n",
    "> - 머신러닝 \n",
    "    - 회귀분석\n",
    "    - 머신러닝 기법\n",
    "> - 딥러닝"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> 텍스트마이닝 단계\n",
    "> Document 를 tokenize & normalize \n",
    "    > - tokenize: 단어단위로 쪼갠다 \n",
    "    > - normalize : 변형되어서 쓰여진 언어들을 원래대로 돌림 \n",
    "    > - ex: go와 went 는 어떻게 보면 같은 단어인데 이대로 두면 잘 모름;\n",
    "    > - POS tagging : 최소 의미 단위로 나누어진 대상에 품사 부착\n",
    "    > - Chunking : POS-tagging 의 결과를 명사구 형용사구 등의 말모듬으로 합침\n",
    "    > - BOW,TFIDF: TOKENIZED 결과를 이용하여 문서를 vector 로 표현 \n",
    "    > - sequence of normlaized words : 순서가 의미가 있는 단어들의 시퀀스 \n",
    "    - [ 'I', have' , ' a', 'pencil']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **NLP** \n",
    "> - document, sentence 등을 sparse vector로 변환    \n",
    "   ( 0이라는 값을 많이 가지고 있는 벡터)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Tokenize**\n",
    "> - documnet 를 sentence의 집합으로 분리 \n",
    "> - 영어의 경우 쉽게 tokenize되나 한글은 구조상 형태소 분석이 필요 (morpheme)\n",
    "\n",
    "> **Text Normalization** \n",
    "> - 동일한 의미의 단어가 다른 형태를 갖는 것을 보완 \n",
    "    > - 다른형태의 단어들을 통일 시켜 표준단어로 변환    \n",
    "> - Stemming (어간추출) \n",
    " - 의미가 아닌 규칙에 의한 변환 현재형 미래형 단수 복수 이런식으로    \n",
    "> - Lemmatization(표제어 추출)\n",
    " - 사전을 이용하여 단어의 원형을 추출 \n",
    "\n",
    "> **POS tagging**   \n",
    "형태소에 대해 품사를 결정하여 할당하는 작업\n",
    "- 형태소분석으로도 번역되기는 하나 / 토근화 정규화 작업에 품사 태깅을 포함한 것으로     \n",
    "보는 것이 타당하다   \n",
    "> [한글 형태소 분석기 도구 - 파이썬](http://konlpy.org/ko/v0.4.3/)\n",
    "\n",
    "> **Chunking**   \n",
    "Chunk 언어학적으로 말모듬 뜻함, 주어와 동사가 없는 두 단어 이상의 집합인 구(phrase)를    \n",
    "의미   \n",
    "Chunking은 주어진 텍스트에서 이와 같은 Chunk를 찾는 것, 각 형태소 값들의 중복 제거\n",
    "- 개체명 인식( NER ) : 개체명은 명사구 사람 날짜 단체 이런것 \n",
    "    - James is working at Disney in London : James -> person   \n",
    "    Disney - > location 이런식으로 아는 것 \n",
    "\n",
    "> **BOW ( Back Of Words )**   \n",
    "원래 문장은 sequence of words 인데 그 시퀀스를 무시하겠다.      \n",
    "즉 단어의 위치(순서)를 무시하고 그냥 그 단어가 문장에 있는지를 알아내겠다.\n",
    "- Vector Space Model      \n",
    "문서를 back of words로 표현 문장안에 벡터로 표현하여 있으면 1 없으면 0 \n",
    "- count vector   \n",
    "단어의 유/무 대신 단어가 문서에 나타난 횟수로 표현 2,3 count 가 weight으로 작용함\n",
    "\n",
    "> **TFIDF( Term Frequency - Inverse Document Frequency )**   \n",
    "자주 등장하지 않는 단어의 weight를 올리고 싶다. 단어의 count를 단어가 나타난 문서의 수로 나누어 줘서 자주등장하지 않는 단어의 weight( count vector 의 문제점) 를 올려줌\n",
    "- tf(d,t) : 문서 d에 단어 t 가 나타난 횟수, =count vector\n",
    "- df(t) : 전체 문서 중에서 단어 t를 포함하는 문서의 수 \n",
    "- idf(t) : df(t)의 역수를 바로 쓸수도있으나 여러 문제 때문에 로그스케일과 스무딩을 적용한 공식을 사용 한다. log(n/(1+df(t)), n은 전체 문서의 수 \n",
    "- 예제 연습 \n",
    "> ![캡처1](https://user-images.githubusercontent.com/52059483/94336965-d86ee880-0021-11eb-9d1b-147505182506.png)\n",
    "\n",
    "> 나이브 베이즈가 SVM 만큼의 성능을 낼 수 도 있다 딥러닝 어려우면 나이브 베이즈 사용 \n",
    "- ex. 어떤 메일이 스팸일 확률/ 스팸인 메일의 단어가 일정 수준 있으면 스팸 / 조건부확률  \n",
    "> ![캡처2](https://user-images.githubusercontent.com/52059483/94342039-4549a900-0049-11eb-8080-2bc6e102a70d.png)\n",
    "\n",
    "> __회귀분석__\n",
    "- 텍스트 마이닝 사용시 무엇이 문제냐면 과적합이 발생하기 쉽고 많은 데이터셋이 필요하다     적은 수로 작동함에도 불구하고 잘 작동하는 편이다 \n",
    "\n",
    "> __릿지 회귀__\n",
    "\n",
    "> __랏쏘 회귀__\n",
    "\n",
    "> __감성분석(Sentiment Analysis)__\n",
    "- label 이 0 이면 부전 1이면 긍정 이런식으로 .. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 텍스트 마이닝의 문제점 \n",
    "\n",
    "> __차원의 저주__\n",
    "- 각데이터 간의 거리가 너무 멀게 위치함\n",
    "- 더 많은 데이터를 사용하면 해결가능 \n",
    "\n",
    "\n",
    "> **단어빈도의 불균형**\n",
    "- zipf's law : 극히 소수의 데이터가 결정적인 영향을 미치게 된다.\n",
    "- 해결방안1 : feature selection (빈도가 높은 단어를 삭제, 심한 경우 50퍼센트 삭제)\n",
    "- 해결방안2 : Boolean Bow 사용 (1이상이면 1로 변환)\n",
    "- 해결방안3 : log사용 하거나 \n",
    "\n",
    "> **단어가 쓰인 순서정보의 손실**\n",
    "- 문맥을 파악해서 글을 읽어야 하는데 통계에 의한 의미 파악 / 순서에 의한 의미 파악이 불가 \n",
    "\n",
    "> ***해결방법*** \n",
    "- feature selection - PCA( principal Component Analysis )   \n",
    "10000개에서 100개를 뽑는데 이 100개를 선형결합을 통해서 뽑는다\n",
    "- LSA(Latent Sentiment Analysis) - SVD (Singular Vector Decomposition)   \n",
    "특이값 분해인데 이해가 안된다 \n",
    "\n",
    "> **Topic Modeling**\n",
    "어떤 토픽을 가지고 그 문서를 썼을까   \n",
    "드라마 시청률 변화 <-> 소셜미디어 토픽 변화   \n",
    "\n",
    "> **Latent Dririchlet Allocation**\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Word Embedding \n",
    "단어에 대한 Vector의 dimension reduction이 목표  \n",
    "word를 fixed length vector 로 어떻게 바꿀까?   \n",
    "BOW는 많은 문서들에 나타난 단어들을 다 쓰게 한다 컬럼에 그러고 한 문서를 파악하고 싶으면 row를 사용 한다 term / document matrix라고 생각해보면댐 \n",
    "> ![캡처](https://user-images.githubusercontent.com/52059483/94343337-5ba83280-0052-11eb-9819-b9f88b8e231a.png)\n",
    "컬럼을 세로로 뜯을경우 단어별로 나오게 된다.만약 두개의 단어가 밀접하다면 두개의 문서에서 사용 되는 빈도도 비슷한 패턴을 가지고 있을 가능성이 높기 때문이다. 이 시점에서는 사용 가능하지만 반복해서는 사용하기 힘들다 \n",
    "\n",
    "> - one hot encoding : 사용된 단어가 100개가 있다고 치자. 가로로 쭉 벡터로 나열한다.   a는 어떻게 표현하냐 a 가 있는 곳만 1로 하고 아니면 다 0으로 한다. 이게 one hot encoding 의도하지않게 거리가 바뀌거나 그러면 비교하기 힘들기 때문에 의도적으로 거리를 똑같이 둔거임   but 이런 경우에 단어의 벡터길이가 되게 길어짐\n",
    "> - Word embedding : 이러한 길어진 단어를 dense vector로 바꿔준다. 일정한 짧은 길이로 바꿔준다. 학습에 의해서 vector의 짧은 길이를 정해준다.이렇게 변화된 것들은 학습 목적 관점에서 단어의 의미를 내포한다고 한다. \n",
    "> ![캡처](https://user-images.githubusercontent.com/52059483/94343435-1f290680-0053-11eb-91f4-c15ecf03b07c.png)\n",
    "\n",
    "> 머신러닝 딥러닝은 아직까지 가변의 길이를 처리하지는 못한다. 입력이 20개 100개 되었다가 이런거는 잘 처리하지 못함 \n",
    "> document를 mxalen을 제한한다 앞 뒤를 잘라내거나 그럼 variable length를 fixed length로 바꾸지만 순서는 살아있는 것임 \n",
    "one hot vector에서 저차원 즉 이차원 행렬로 바뀌게 됨 (maxlen,reduced_dim저차원)\n",
    "\n",
    "> **word2vec**\n",
    "- 단어간의 유사성을 통해 연산을 한다 \n",
    "- 문장에 나타난 단어들의 순서를 이용해 word embedding을 수행 두 가지 목적이 있음\n",
    ">![캡처5](https://user-images.githubusercontent.com/52059483/94343580-01a86c80-0054-11eb-9ce9-684b9d37d8f4.png) \n",
    "![캡처6](https://user-images.githubusercontent.com/52059483/94343588-15ec6980-0054-11eb-9bff-f0f6246c5306.png)\n",
    "> green을 목표로해서 weight를 주는 방법 CBOW ( green을 빼고 나머지 것으로 green 예측)   같은 위치에 자주오는 단어 white green 이런거는 비슷하다고 판단 \n",
    "> green을 input으로 해서 나머지를 찾음 skip ngram \n",
    "\n",
    "> **ELMo**\n",
    ">![캡처](https://user-images.githubusercontent.com/52059483/94343663-94490b80-0054-11eb-92e4-b7d1b1ccf653.png)\n",
    "- 워트투벡과 어느정도 비슷하기는 함 \n",
    "- 다른점은 word2vec 이나 glove는 fixed embedding이다. \n",
    "    - ex 배의 경우 배가 vector로 fixed 되있지만 배를 타고가다 배를 먹다가 같은 값이됨\n",
    "- 문맥을 파악하기 위해서 나온게 엘무 but 딥러닝을 제대로 이해해야만 잘 쓸 수 있다.     \n",
    "양방향을 다쓴다\n",
    "- Transfer Learning \n",
    "\n",
    "> **Document Embedding**\n",
    "- document 에 대해서 dense vector를 생성하는 모형 word2vec 모형가 비슷 하고 그것을 기본으로 하고 있기는 함 \n",
    "\n",
    ">  **RBM( Restricted Boltzmann Machine)**\n",
    "- 지금은 안쓰이지만 딥러닝의 역사를 열어줌\n",
    "- 차원의 저주를 해결하기위해 만들어짐/ 차원을 변경하면서 원래의 정보량을 유지해줌 \n",
    "\n",
    "> **Autoencoder**\n",
    "- RBM과 유사하나 encoder 로 차원을 축소하고 decoder로 다시 복원시키면서 차원을 줄이고 늘리면서 학습하는것. 지금도 많이 쓰임. 작동방식은 PCA와 유사 \n",
    "![캡처9](https://user-images.githubusercontent.com/52059483/94343775-8942ab00-0055-11eb-8d5d-a406a7c3364a.png)\n",
    "\n",
    "> **Context의 파악**   \n",
    "> 1. **N-gram**      \n",
    "문맥을 파악하기 위한 전통적인 방법\n",
    "    - 문장을 두개 단위로 쪼갠다\n",
    "        - the future / future depends / depends on 이런식으로\n",
    "            - unigram, bigram(2개씩), trigram 보통 unigram에 bigram trigram을 추가하면서 feature의 수를 증가시킨다.\n",
    "            \n",
    "> 2.**딥러닝 - RNN**      \n",
    "문장을 단어들의 sequence혹은 series로 처리한다.   \n",
    "sequence 정보를 기억하는 방법은 x 는 embedding 된 word들을 순서대로 나열하는 것이다.   \n",
    "hidden node가 x 뿐아니라 그 전 hiddden node 로부터도 입력을 받는 형태 \n",
    ">LSTM( Long Short Term Memory )\n",
    "- RNN의 문제 : 문장이 길수록 층이 깊은 형태를 갖게 되서 경사가 소실, 앞 단어 정보가 학습 안됨\n",
    "- LSTM 직통 경로를 만들어 해결 \n",
    "\n",
    "> 3.**Bi - LSTM**\n",
    "- 어떤 단어는 앞에서 뒤로 영향을 미치기도 하지만 어떤 거는 뒤에 있는게 앞에 영향을 미침(독일어)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **sequence to sequence**   \n",
    "- 번역 같은 문제 때문에 나온 것임 ex 영어 문장에서 독일어 문장 시퀀스 투 시퀀스 임 \n",
    "- 문맥정보를 계속 인코딩하고 쌓인 애들을 풀어내면서 예측 해야함     LSTM ENCODER TO DECODER ( Chatbot, Summarize)         \n",
    "\n",
    "> **Attention**\n",
    "- 출력에 나온 어떤 단어는 입력에 있는 특정 단어들에 민감한 것에 착안\n",
    "- 입력의 단어들로부터 출력 단어에 직접 링크를 만듦 \n",
    "![캡처10](https://user-images.githubusercontent.com/52059483/94355229-6ea41c80-00bd-11eb-90d2-41508141f66b.png)\n",
    "\n",
    "> **Transformer(Self-attention)**\n",
    "- 입력 단어들 끼리에도 상호 연관성이 있는 것에 착안 \n",
    "- 입력 출력으로 attention 이외에 입력 단어들 간의 attention , 입력 + 출력 -> 출력으로의 attention을 추가 \n",
    "\n",
    "> **BERT**\n",
    "- 양방향 transformer 인코더를 사용 \n",
    "- GPT는 decoder만을 썼음 attention이 한쪽 방향으로만 감 앞에 생성된 애들로만 감 한 방향으로만 간다. \n",
    "- 거의 모든 분야에서 top score를 받음 \n",
    "- segment, position embedding 을 사용한다. \n",
    "![캡처12](https://user-images.githubusercontent.com/52059483/94355298-086bc980-00be-11eb-879e-d366ad5c4fcb.png)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
